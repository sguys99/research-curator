# ë°ì´í„° ì²˜ë¦¬ ëª¨ë“ˆ (Processors)

LLMì„ í™œìš©í•˜ì—¬ ìˆ˜ì§‘ëœ ì•„í‹°í´ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [ëª¨ë“ˆ êµ¬ì„±](#ëª¨ë“ˆ-êµ¬ì„±)
- [ì‚¬ìš©ë²•](#ì‚¬ìš©ë²•)
- [API ë ˆí¼ëŸ°ìŠ¤](#api-ë ˆí¼ëŸ°ìŠ¤)
- [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)
- [ì˜ˆì œ](#ì˜ˆì œ)

---

## ê°œìš”

ProcessorsëŠ” 5ê°œì˜ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

| í”„ë¡œì„¸ì„œ | ê¸°ëŠ¥ | ì¶œë ¥ |
|---------|------|------|
| **ArticleSummarizer** | í•œêµ­ì–´/ì˜ì–´ ìš”ì•½ ìƒì„± | ìš”ì•½ ë¬¸ìì—´ |
| **ImportanceEvaluator** | ì¤‘ìš”ë„ í‰ê°€ (LLM + ë©”íƒ€ë°ì´í„°) | 0.0-1.0 ì ìˆ˜ |
| **ContentClassifier** | ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ + ë©”íƒ€ë°ì´í„° ì¶”ì¶œ | ì¹´í…Œê³ ë¦¬, í‚¤ì›Œë“œ ë“± |
| **TextEmbedder** | ì„ë² ë”© ë²¡í„° ìƒì„± | 1536ì°¨ì› ë²¡í„° |
| **ProcessingPipeline** | í†µí•© íŒŒì´í”„ë¼ì¸ (ëª¨ë“  ì²˜ë¦¬ ìë™í™”) | ProcessedArticle |

### ì£¼ìš” íŠ¹ì§•

- âœ… **ë¹„ë™ê¸° ì²˜ë¦¬**: asyncio ê¸°ë°˜ ê³ ì„±ëŠ¥
- âœ… **ë°°ì¹˜ ì²˜ë¦¬**: ì—¬ëŸ¬ ì•„í‹°í´ ë™ì‹œ ì²˜ë¦¬
- âœ… **ì—ëŸ¬ í•¸ë“¤ë§**: ë¶€ë¶„ ì‹¤íŒ¨ í—ˆìš©
- âœ… **ìºì‹±**: ì„ë² ë”© ìºì‹œë¡œ ë¹„ìš© ì ˆê°
- âœ… **íƒ€ì… ì•ˆì „**: ì™„ì „í•œ íƒ€ì… íŒíŠ¸

---

## ëª¨ë“ˆ êµ¬ì„±

```
src/app/processors/
â”œâ”€â”€ __init__.py          # íŒ¨í‚¤ì§€ ì´ˆê¸°í™”
â”œâ”€â”€ summarizer.py        # ìš”ì•½ ìƒì„±
â”œâ”€â”€ evaluator.py         # ì¤‘ìš”ë„ í‰ê°€
â”œâ”€â”€ classifier.py        # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
â”œâ”€â”€ embedder.py          # ì„ë² ë”© ìƒì„±
â””â”€â”€ pipeline.py          # í†µí•© íŒŒì´í”„ë¼ì¸
```

---

## ì‚¬ìš©ë²•

### 1. ArticleSummarizer

```python
from src.app.processors import ArticleSummarizer

# ì´ˆê¸°í™”
summarizer = ArticleSummarizer(
    provider="openai",
    temperature=0.3  # ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„±â†‘
)

# ë‹¨ì¼ ìš”ì•½
summary = await summarizer.summarize(
    title="Attention Is All You Need",
    content="We propose the Transformer...",
    language="ko",      # "ko" ë˜ëŠ” "en"
    length="medium"     # "short", "medium", "long"
)

# ë°°ì¹˜ ìš”ì•½
articles = [
    {"title": "Paper 1", "content": "..."},
    {"title": "Paper 2", "content": "..."},
]
summaries = await summarizer.batch_summarize(
    articles,
    language="ko",
    length="short"
)
```

**ìš”ì•½ ê¸¸ì´ ì˜µì…˜**:
- `short`: 2-3ë¬¸ì¥ (ê°„ê²°í•œ ìš”ì•½)
- `medium`: 3-5ë¬¸ì¥ (í•µì‹¬ ì•„ì´ë””ì–´ + ì£¼ìš” ë°œê²¬)
- `long`: 6-8ë¬¸ì¥ (ë°°ê²½, ë°©ë²•ë¡ , ê²°ê³¼, ì˜ë¯¸)

---

### 2. ImportanceEvaluator

```python
from src.app.processors import ImportanceEvaluator

# ì´ˆê¸°í™”
evaluator = ImportanceEvaluator(
    provider="openai",
    temperature=0.2,
    llm_weight=0.7,        # LLM í‰ê°€ ê°€ì¤‘ì¹˜
    metadata_weight=0.3    # ë©”íƒ€ë°ì´í„° ê°€ì¤‘ì¹˜
)

# ë‹¨ì¼ í‰ê°€
result = await evaluator.evaluate(
    title="GPT-4 Technical Report",
    content="GPT-4 is a large multimodal model...",
    metadata={
        "citations": 5000,
        "year": 2023,
        "source_name": "OpenAI"
    }
)

print(result["final_score"])  # 0.89
print(result["innovation"])   # 0.95
print(result["relevance"])    # 0.90
print(result["impact"])       # 0.85
print(result["timeliness"])   # 0.80
```

**í‰ê°€ ê¸°ì¤€**:
- `innovation` (30%): í˜ì‹ ì„±, ìƒˆë¡œìš´ ì•„ì´ë””ì–´
- `relevance` (25%): AI ë¶„ì•¼ ê´€ë ¨ì„±, ì‹¤ìš©ì  ê°€ì¹˜
- `impact` (30%): í•™ê³„/ì‚°ì—…ê³„ ì˜í–¥ë ¥
- `timeliness` (15%): ì‹œì˜ì„±, ìµœì‹  íŠ¸ë Œë“œ

**ë©”íƒ€ë°ì´í„° í‰ê°€ ìš”ì†Œ**:
- ì¸ìš©ìˆ˜ (citations)
- ì¶œì²˜ ì‹ ë¢°ë„ (source_name)
- ìµœì‹ ì„± (year, publication_date)

---

### 3. ContentClassifier

```python
from src.app.processors import ContentClassifier

# ì´ˆê¸°í™”
classifier = ContentClassifier(
    provider="openai",
    temperature=0.1  # ë§¤ìš° ë‚®ê²Œ ì„¤ì • (ì¼ê´€ì„± ê·¹ëŒ€í™”)
)

# ë‹¨ì¼ ë¶„ë¥˜
result = await classifier.classify(
    title="Attention Is All You Need",
    content="We propose the Transformer...",
    source_name="arXiv",
    url="https://arxiv.org/abs/1706.03762"
)

print(result["category"])         # "paper"
print(result["confidence"])       # 0.95
print(result["research_field"])   # "Natural Language Processing"
print(result["keywords"])         # ["Transformer", "Attention", "NMT"]
```

**ì¹´í…Œê³ ë¦¬**:
- `paper`: í•™ìˆ  ë…¼ë¬¸ (arXiv, í•™íšŒ, ì €ë„)
- `news`: ë‰´ìŠ¤ ê¸°ì‚¬ (ì–¸ë¡ ì‚¬, í…Œí¬ ë¸”ë¡œê·¸)
- `report`: ì—°êµ¬ ë¦¬í¬íŠ¸ (ê¸°ì—…, ì—°êµ¬ì†Œ ë³´ê³ ì„œ)
- `blog`: ê°œì¸ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸
- `other`: ê¸°íƒ€

**ë°˜í™˜ ë°ì´í„°**:
```python
{
    "category": "paper",
    "confidence": 0.95,
    "keywords": ["Transformer", "Attention"],
    "research_field": "Natural Language Processing",
    "sub_fields": ["Machine Translation", "Neural Networks"],
    "reasoning": "arXivì— ê²Œì¬ëœ í•™ìˆ  ë…¼ë¬¸..."
}
```

---

### 4. TextEmbedder

```python
from src.app.processors import TextEmbedder

# ì´ˆê¸°í™”
embedder = TextEmbedder(
    use_cache=True  # ìºì‹± í™œì„±í™”
)

# ë‹¨ì¼ ì„ë² ë”©
embedding = await embedder.embed("Attention Is All You Need")
print(len(embedding))  # 1536

# ë°°ì¹˜ ì„ë² ë”©
texts = ["Text 1", "Text 2", "Text 3"]
embeddings = await embedder.batch_embed(texts)
print(len(embeddings))  # 3

# ì•„í‹°í´ ì„ë² ë”© (ì œëª© + ìš”ì•½ + ë‚´ìš©)
embedding = await embedder.embed_article_async(
    title="GPT-4",
    content="GPT-4 is a large multimodal model...",
    summary="GPT-4ëŠ” ëŒ€ê·œëª¨ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ì…ë‹ˆë‹¤."
)
```

**ìºì‹œ ê´€ë¦¬**:
```python
# ìºì‹œ í¬ê¸° í™•ì¸
print(embedder.get_cache_size())

# ìºì‹œ ì´ˆê¸°í™”
embedder.clear_cache()
```

---

## API ë ˆí¼ëŸ°ìŠ¤

### ArticleSummarizer

#### `summarize(title, content, language, length, max_tokens) -> str`

ë‹¨ì¼ ì•„í‹°í´ ìš”ì•½ ìƒì„±

**Parameters**:
- `title` (str): ì•„í‹°í´ ì œëª©
- `content` (str): ì•„í‹°í´ ë‚´ìš©
- `language` (Literal["ko", "en"]): ìš”ì•½ ì–¸ì–´ (ê¸°ë³¸: "ko")
- `length` (Literal["short", "medium", "long"]): ìš”ì•½ ê¸¸ì´ (ê¸°ë³¸: "medium")
- `max_tokens` (int): ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸: 500)

**Returns**: `str` - ìš”ì•½ ë¬¸ìì—´

#### `batch_summarize(articles, language, length, max_tokens) -> List[str]`

ì—¬ëŸ¬ ì•„í‹°í´ ë™ì‹œ ìš”ì•½

**Parameters**:
- `articles` (List[dict]): ì•„í‹°í´ ë¦¬ìŠ¤íŠ¸ `[{"title": "...", "content": "..."}, ...]`
- ë‚˜ë¨¸ì§€ íŒŒë¼ë¯¸í„°ëŠ” `summarize()`ì™€ ë™ì¼

**Returns**: `List[str]` - ìš”ì•½ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸

---

### ImportanceEvaluator

#### `evaluate(title, content, metadata, max_tokens) -> Dict[str, float]`

ë‹¨ì¼ ì•„í‹°í´ ì¤‘ìš”ë„ í‰ê°€

**Parameters**:
- `title` (str): ì•„í‹°í´ ì œëª©
- `content` (str): ì•„í‹°í´ ë‚´ìš©
- `metadata` (Optional[Dict[str, Any]]): ë©”íƒ€ë°ì´í„° (ê¸°ë³¸: None)
- `max_tokens` (int): ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸: 500)

**Returns**: `Dict[str, float]`
```python
{
    "innovation": 0.0-1.0,
    "relevance": 0.0-1.0,
    "impact": 0.0-1.0,
    "timeliness": 0.0-1.0,
    "reasoning": "í‰ê°€ ê·¼ê±°",
    "llm_score": 0.0-1.0,
    "metadata_score": 0.0-1.0,
    "final_score": 0.0-1.0
}
```

---

### ContentClassifier

#### `classify(title, content, source_name, url, max_tokens) -> Dict[str, Any]`

ë‹¨ì¼ ì•„í‹°í´ ë¶„ë¥˜ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

**Parameters**:
- `title` (str): ì•„í‹°í´ ì œëª©
- `content` (str): ì•„í‹°í´ ë‚´ìš©
- `source_name` (str): ì†ŒìŠ¤ ì´ë¦„ (ê¸°ë³¸: "")
- `url` (str): ì›ë¬¸ URL (ê¸°ë³¸: "")
- `max_tokens` (int): ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸: 500)

**Returns**: `Dict[str, Any]` (ìœ„ ì‚¬ìš©ë²• ì„¹ì…˜ ì°¸ì¡°)

---

### TextEmbedder

#### `embed(text) -> List[float]`

ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±

**Parameters**:
- `text` (str): ì„ë² ë”©í•  í…ìŠ¤íŠ¸

**Returns**: `List[float]` - 1536ì°¨ì› ë²¡í„°

#### `batch_embed(texts, batch_size) -> List[List[float]]`

ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ë™ì‹œ ì„ë² ë”©

**Parameters**:
- `texts` (List[str]): í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
- `batch_size` (int): ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: 100)

**Returns**: `List[List[float]]` - ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸

---

## ì„±ëŠ¥ ìµœì í™”

### 1. ë³‘ë ¬ ì²˜ë¦¬

```python
import asyncio

# âŒ ìˆœì°¨ ì²˜ë¦¬ (ëŠë¦¼)
for article in articles:
    summary = await summarizer.summarize(...)

# âœ… ë³‘ë ¬ ì²˜ë¦¬ (ë¹ ë¦„)
summaries = await summarizer.batch_summarize(articles)
```

### 2. ì„ë² ë”© ìºì‹±

```python
# ìºì‹± í™œì„±í™”ë¡œ ì¤‘ë³µ ì„ë² ë”© ë°©ì§€
embedder = TextEmbedder(use_cache=True)

# ê°™ì€ í…ìŠ¤íŠ¸ëŠ” ìºì‹œì—ì„œ ê°€ì ¸ì˜´ (API í˜¸ì¶œ ì—†ìŒ)
emb1 = await embedder.embed("same text")
emb2 = await embedder.embed("same text")  # ìºì‹œ íˆíŠ¸
```

### 3. ì˜¨ë„ ì„¤ì • ìµœì í™”

```python
# ìš”ì•½: ë‹¤ì–‘ì„± í•„ìš” â†’ ì˜¨ë„ ë†’ê²Œ
summarizer = ArticleSummarizer(temperature=0.3)

# ë¶„ë¥˜: ì¼ê´€ì„± í•„ìš” â†’ ì˜¨ë„ ë‚®ê²Œ
classifier = ContentClassifier(temperature=0.1)

# í‰ê°€: ê· í˜• â†’ ì˜¨ë„ ì¤‘ê°„
evaluator = ImportanceEvaluator(temperature=0.2)
```

---

## ì˜ˆì œ

### ì „ì²´ íŒŒì´í”„ë¼ì¸

```python
from src.app.processors import (
    ArticleSummarizer,
    ImportanceEvaluator,
    ContentClassifier,
    TextEmbedder,
)

async def process_article(article):
    """ë‹¨ì¼ ì•„í‹°í´ ì „ì²´ ì²˜ë¦¬"""

    # 1. ìš”ì•½ ìƒì„±
    summarizer = ArticleSummarizer()
    summary = await summarizer.summarize(
        title=article["title"],
        content=article["content"],
        language="ko",
        length="medium"
    )

    # 2. ì¤‘ìš”ë„ í‰ê°€
    evaluator = ImportanceEvaluator()
    eval_result = await evaluator.evaluate(
        title=article["title"],
        content=article["content"],
        metadata=article.get("metadata", {})
    )

    # 3. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
    classifier = ContentClassifier()
    class_result = await classifier.classify(
        title=article["title"],
        content=article["content"],
        source_name=article.get("source_name", ""),
        url=article.get("url", "")
    )

    # 4. ì„ë² ë”© ìƒì„±
    embedder = TextEmbedder()
    embedding = await embedder.embed_article_async(
        title=article["title"],
        content=article["content"],
        summary=summary
    )

    return {
        "title": article["title"],
        "summary": summary,
        "importance_score": eval_result["final_score"],
        "category": class_result["category"],
        "keywords": class_result["keywords"],
        "embedding": embedding
    }
```

### ë°°ì¹˜ ì²˜ë¦¬ (ìµœì í™”)

```python
async def process_batch(articles):
    """ì—¬ëŸ¬ ì•„í‹°í´ ë³‘ë ¬ ì²˜ë¦¬ (ìµœê³  ì„±ëŠ¥)"""

    # 1. ìš”ì•½, í‰ê°€, ë¶„ë¥˜ ë™ì‹œ ì‹¤í–‰
    summarizer = ArticleSummarizer()
    evaluator = ImportanceEvaluator()
    classifier = ContentClassifier()

    summaries, eval_results, class_results = await asyncio.gather(
        summarizer.batch_summarize(articles),
        evaluator.batch_evaluate(articles),
        classifier.batch_classify(articles)
    )

    # 2. ì„ë² ë”© ìƒì„± (ìš”ì•½ ì‚¬ìš©)
    embedder = TextEmbedder()
    embedding_texts = [
        embedder.embed_article(
            title=article["title"],
            content=article["content"],
            summary=summary
        )
        for article, summary in zip(articles, summaries)
    ]
    embeddings = await embedder.batch_embed(embedding_texts)

    # 3. ê²°ê³¼ ê²°í•©
    results = []
    for article, summary, eval_res, class_res, embedding in zip(
        articles, summaries, eval_results, class_results, embeddings
    ):
        results.append({
            "title": article["title"],
            "summary": summary,
            "importance_score": eval_res["final_score"],
            "category": class_res["category"],
            "keywords": class_res["keywords"],
            "embedding": embedding
        })

    return results
```

---

## ğŸš€ ProcessingPipeline (í†µí•© íŒŒì´í”„ë¼ì¸)

**ê°€ì¥ ê¶Œì¥í•˜ëŠ” ì‚¬ìš©ë²•**: ëª¨ë“  í”„ë¡œì„¸ì„œë¥¼ ìë™ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” í†µí•© íŒŒì´í”„ë¼ì¸

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from src.app.processors import ProcessingPipeline

# íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
pipeline = ProcessingPipeline(
    provider="openai",
    summary_length="medium",  # short, medium, long
    summary_language="ko"     # ko, en
)

# ë‹¨ì¼ ì•„í‹°í´ ì²˜ë¦¬ (ëª¨ë“  ë‹¨ê³„ ìë™ ì‹¤í–‰)
result = await pipeline.process_article(
    title="Attention Is All You Need",
    content="We propose the Transformer...",
    url="https://arxiv.org/abs/1706.03762",
    source_name="arXiv",
    metadata={"year": 2017, "citations": 50000}
)

# ê²°ê³¼: ProcessedArticle ê°ì²´
print(result.summary)           # í•œêµ­ì–´ ìš”ì•½
print(result.importance_score)  # 0.94
print(result.category)          # "paper"
print(result.keywords)          # ["Transformer", "Attention", ...]
print(result.embedding)         # [0.1, 0.2, ...] (1536 dims)
```

### ë°°ì¹˜ ì²˜ë¦¬ (ìµœê³  ì„±ëŠ¥)

```python
articles = [
    {"title": "Paper 1", "content": "...", "url": "...", "metadata": {...}},
    {"title": "Paper 2", "content": "...", "url": "...", "metadata": {...}},
    {"title": "Paper 3", "content": "...", "url": "...", "metadata": {...}},
]

# ë³‘ë ¬ ì²˜ë¦¬ (max_concurrentë¡œ ë™ì‹œ ì‹¤í–‰ ì œí•œ)
results = await pipeline.process_batch(
    articles,
    max_concurrent=5  # ë™ì‹œì— 5ê°œê¹Œì§€ ì²˜ë¦¬
)

# 5ê°œ ì•„í‹°í´ ì²˜ë¦¬: ~8ì´ˆ (í‰ê·  1.7ì´ˆ/ì•„í‹°í´)
```

### ProcessedArticle ë°ì´í„° êµ¬ì¡°

```python
@dataclass
class ProcessedArticle:
    # ì›ë³¸ ë°ì´í„°
    title: str
    content: str
    url: str
    source_name: str
    source_type: str

    # ì²˜ë¦¬ ê²°ê³¼
    summary: str                 # í•œêµ­ì–´ ìš”ì•½
    importance_score: float      # ìµœì¢… ì¤‘ìš”ë„ (0.0-1.0)
    category: str                # paper/news/report/blog/other
    keywords: List[str]          # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
    research_field: str          # ì—°êµ¬ ë¶„ì•¼
    embedding: List[float]       # 1536ì°¨ì› ë²¡í„°

    # ìƒì„¸ í‰ê°€
    innovation_score: float      # í˜ì‹ ì„± (0.0-1.0)
    relevance_score: float       # ê´€ë ¨ì„± (0.0-1.0)
    impact_score: float          # ì˜í–¥ë ¥ (0.0-1.0)
    timeliness_score: float      # ì‹œì˜ì„± (0.0-1.0)

    # ë©”íƒ€ë°ì´í„°
    metadata: Dict[str, Any]
    processed_at: datetime
```

### ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜

```python
# ìƒìœ„ Nê°œ ì•„í‹°í´ (ì¤‘ìš”ë„ìˆœ)
top_articles = pipeline.get_top_articles(results, top_n=5)

# ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§
papers = pipeline.filter_by_category(results, category="paper")

# ì ìˆ˜ ê¸°ì¤€ í•„í„°ë§
high_quality = pipeline.filter_by_score(results, min_score=0.7)

# í†µê³„ ì •ë³´
stats = pipeline.get_statistics(results)
print(stats)
# {
#     "total": 5,
#     "category_distribution": {"paper": 4, "news": 1},
#     "average_score": 0.89,
#     "max_score": 0.94,
#     "min_score": 0.85,
#     "high_quality_count": 5
# }
```

### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

| ì‘ì—… | ì‹œê°„ | ë¹„ê³  |
|-----|------|------|
| ë‹¨ì¼ ì•„í‹°í´ ì²˜ë¦¬ | ~3.7ì´ˆ | ìš”ì•½+í‰ê°€+ë¶„ë¥˜+ì„ë² ë”© |
| ë°°ì¹˜ 5ê°œ ì²˜ë¦¬ | ~8.6ì´ˆ | ë³‘ë ¬ ì²˜ë¦¬ (í‰ê·  1.7ì´ˆ/ê°œ) |
| ë°°ì¹˜ 10ê°œ ì²˜ë¦¬ | ~15ì´ˆ | max_concurrent=5 |

**ìµœì í™” íŒ**:
- `max_concurrent` ì¡°ì • (ê¶Œì¥: 3-5)
- `summary_length="short"` ì‚¬ìš© ì‹œ ë” ë¹ ë¦„
- ì„ë² ë”© ìºì‹± í™œì„±í™” (ê¸°ë³¸ê°’)

---

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- [í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ ì‹œìŠ¤í…œ](./PROMPTS.md)
- [LLM í†µí•© ê°€ì´ë“œ](./LLM_INTEGRATION.md)
- [API ë¬¸ì„œ](./API.md)

---

**ì‘ì„±ì¼**: 2025-12-03
**ë²„ì „**: 1.1.0 (íŒŒì´í”„ë¼ì¸ ì¶”ê°€)
