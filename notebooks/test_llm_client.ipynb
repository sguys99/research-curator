{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Client Testing Notebook\n",
    "\n",
    "이 노트북은 LLM 클라이언트의 기능을 테스트하고 다양한 사용 사례를 시연합니다.\n",
    "\n",
    "## Setup\n",
    "먼저 필요한 모듈을 import합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport sys\nfrom pathlib import Path\n\nfrom dotenv import load_dotenv\n\nfrom src.app.llm import LLMClient\n\n# Add project root to path\nproject_root = Path.cwd().parent\nsys.path.insert(0, str(project_root))\n\n# Load environment variables\nload_dotenv(project_root / \".env\")\n\nprint(\"✓ Setup complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Chat Completion\n",
    "\n",
    "기본적인 채팅 완성 기능을 테스트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 클라이언트 생성\n",
    "openai_client = LLMClient(provider=\"openai\", temperature=0.7)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI research assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are the key trends in AI research in 2024?\"},\n",
    "]\n",
    "\n",
    "response = openai_client.chat_completion(messages, max_tokens=300)\n",
    "print(\"OpenAI Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Claude Chat Completion\n",
    "\n",
    "Claude API를 사용한 채팅 완성을 테스트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 클라이언트 생성 (API 키가 설정된 경우에만)\n",
    "try:\n",
    "    claude_client = LLMClient(provider=\"claude\", temperature=0.7)\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": \"What are the key trends in AI research in 2024?\"}]\n",
    "\n",
    "    response = claude_client.chat_completion(messages, max_tokens=300)\n",
    "    print(\"Claude Response:\")\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"Claude API not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. JSON Response Format\n",
    "\n",
    "JSON 형식의 응답을 요청하고 파싱합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = LLMClient(provider=\"openai\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in JSON format.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"\n",
    "        Analyze this article title and return a JSON object with:\n",
    "        - category: one of (paper, news, report)\n",
    "        - importance_score: 0.0 to 1.0\n",
    "        - keywords: list of 3-5 relevant keywords\n",
    "        - summary: brief one-line summary\n",
    "\n",
    "        Title: \"GPT-5 Achieves Human-Level Performance on Complex Reasoning Tasks\"\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "response = client.chat_completion(messages, response_format=\"json\", max_tokens=300)\n",
    "print(\"JSON Response:\")\n",
    "print(response)\n",
    "\n",
    "# Parse JSON\n",
    "data = json.loads(response)\n",
    "print(\"\\nParsed Data:\")\n",
    "print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Article Summarization (Korean)\n",
    "\n",
    "논문을 한국어로 요약하는 기능을 테스트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = LLMClient(provider=\"openai\", temperature=0.5)\n",
    "\n",
    "article_text = \"\"\"\n",
    "Researchers at MIT have developed a new neural network architecture\n",
    "that combines the benefits of transformers with the efficiency of\n",
    "convolutional neural networks. The hybrid model, dubbed TransConv,\n",
    "achieves state-of-the-art results on image classification tasks while\n",
    "requiring 40% less computational resources than traditional transformer\n",
    "models. The key innovation lies in the selective attention mechanism\n",
    "that adaptively chooses between local and global feature processing\n",
    "based on the input characteristics.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI research expert. Summarize articles in Korean.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"다음 논문 내용을 한국어로 3-4문장으로 요약해주세요:\\n\\n{article_text}\",\n",
    "    },\n",
    "]\n",
    "\n",
    "summary = client.chat_completion(messages, max_tokens=500)\n",
    "print(\"Korean Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embedding Generation\n",
    "\n",
    "텍스트 임베딩을 생성하고 유사도를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "client = LLMClient(provider=\"openai\")\n",
    "\n",
    "# 여러 텍스트의 임베딩 생성\n",
    "texts = [\n",
    "    \"Transformer architecture in deep learning\",\n",
    "    \"Attention mechanism for neural networks\",\n",
    "    \"Reinforcement learning for robotics\",\n",
    "    \"Computer vision using CNNs\",\n",
    "]\n",
    "\n",
    "embeddings = [client.generate_embedding(text) for text in texts]\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings\")\n",
    "print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
    "print(f\"First embedding (first 5 values): {embeddings[0][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도 계산\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "\n",
    "print(\"\\nCosine Similarities:\")\n",
    "for i, text_i in enumerate(texts):\n",
    "    for j, text_j in enumerate(texts):\n",
    "        if i < j:\n",
    "            similarity = cosine_similarity(embeddings[i], embeddings[j])\n",
    "            print(f\"'{text_i}' vs '{text_j}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Async Operations\n",
    "\n",
    "비동기 작업을 테스트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def test_async_operations():\n",
    "    client = LLMClient(provider=\"openai\")\n",
    "\n",
    "    # Async chat completion\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is machine learning in one sentence?\"},\n",
    "    ]\n",
    "\n",
    "    response = await client.achat_completion(messages, max_tokens=100)\n",
    "    print(\"Async Chat Response:\")\n",
    "    print(response)\n",
    "    print()\n",
    "\n",
    "    # Async embedding\n",
    "    embedding = await client.agenerate_embedding(\"Deep learning for NLP\")\n",
    "    print(f\"Async Embedding dimension: {len(embedding)}\")\n",
    "    print(f\"First 5 values: {embedding[:5]}\")\n",
    "\n",
    "\n",
    "# Run async function\n",
    "await test_async_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Batch Processing\n",
    "\n",
    "여러 문서를 동시에 처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_articles_batch(articles):\n",
    "    \"\"\"여러 논문을 동시에 요약\"\"\"\n",
    "    client = LLMClient(provider=\"openai\", temperature=0.5)\n",
    "\n",
    "    async def summarize_one(article):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Summarize the article in Korean in 2 sentences.\"},\n",
    "            {\"role\": \"user\", \"content\": article},\n",
    "        ]\n",
    "        return await client.achat_completion(messages, max_tokens=200)\n",
    "\n",
    "    # 모든 요약을 동시에 실행\n",
    "    summaries = await asyncio.gather(*[summarize_one(article) for article in articles])\n",
    "    return summaries\n",
    "\n",
    "\n",
    "# Test batch processing\n",
    "articles = [\n",
    "    \"GPT-5 shows remarkable improvements in reasoning capabilities.\",\n",
    "    \"New transformer architecture reduces computational costs by 40%.\",\n",
    "    \"Reinforcement learning breakthrough enables better robot control.\",\n",
    "]\n",
    "\n",
    "summaries = await process_articles_batch(articles)\n",
    "\n",
    "print(\"Batch Processing Results:\")\n",
    "for i, (article, summary) in enumerate(zip(articles, summaries, strict=False), 1):\n",
    "    print(f\"\\n{i}. Original: {article}\")\n",
    "    print(f\"   Summary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Temperature Comparison\n",
    "\n",
    "다양한 temperature 값에 따른 응답 차이를 비교합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [0.0, 0.5, 1.0]\n",
    "prompt = \"Explain neural networks in one sentence.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "print(\"Temperature Comparison:\")\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "for temp in temperatures:\n",
    "    client = LLMClient(provider=\"openai\", temperature=temp)\n",
    "    response = client.chat_completion(messages, max_tokens=100)\n",
    "    print(f\"Temperature {temp}:\")\n",
    "    print(f\"  {response}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Handling\n",
    "\n",
    "에러 처리를 테스트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test invalid provider\n",
    "try:\n",
    "    invalid_client = LLMClient(provider=\"invalid\")\n",
    "except ValueError as e:\n",
    "    print(f\"✓ Caught expected error for invalid provider: {e}\")\n",
    "\n",
    "# Test with very large max_tokens (should be handled by API)\n",
    "try:\n",
    "    client = LLMClient(provider=\"openai\")\n",
    "    messages = [{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    "    response = client.chat_completion(messages, max_tokens=100000)\n",
    "    print(\"\\nResponse with large max_tokens received (API handled it)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with large max_tokens: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Use Case: Research Article Analysis\n",
    "\n",
    "실제 사용 사례: 연구 논문 분석 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def analyze_research_article(title, abstract):\n",
    "    \"\"\"연구 논문을 분석하고 메타데이터를 추출\"\"\"\n",
    "    client = LLMClient(provider=\"openai\", temperature=0.3)\n",
    "\n",
    "    # 1. 카테고리 분류 및 중요도 평가\n",
    "    analysis_messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an AI research expert. Analyze research articles and respond in JSON.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "            Analyze this research article and return JSON with:\n",
    "            - category: one of (paper, news, report)\n",
    "            - importance_score: 0.0 to 1.0\n",
    "            - keywords: list of 5 keywords\n",
    "            - field: research field (e.g., \"Computer Vision\", \"NLP\", \"Robotics\")\n",
    "\n",
    "            Title: {title}\n",
    "            Abstract: {abstract}\n",
    "            \"\"\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # 2. 한국어 요약\n",
    "    summary_content = (\n",
    "        f\"다음 논문을 한국어로 3-4문장으로 요약해주세요:\\n\\n\" f\"Title: {title}\\n\\nAbstract: {abstract}\"\n",
    "    )\n",
    "    summary_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI research expert. Summarize in Korean.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": summary_content,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # 3. 임베딩 생성\n",
    "    embedding_text = f\"{title} {abstract}\"\n",
    "\n",
    "    # 모두 비동기로 동시 실행\n",
    "    analysis_task = client.achat_completion(analysis_messages, response_format=\"json\", max_tokens=300)\n",
    "    summary_task = client.achat_completion(summary_messages, max_tokens=500)\n",
    "    embedding_task = client.agenerate_embedding(embedding_text)\n",
    "\n",
    "    analysis_response, summary, embedding = await asyncio.gather(\n",
    "        analysis_task, summary_task, embedding_task\n",
    "    )\n",
    "\n",
    "    # 결과 파싱\n",
    "    analysis = json.loads(analysis_response)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"category\": analysis.get(\"category\"),\n",
    "        \"importance_score\": analysis.get(\"importance_score\"),\n",
    "        \"keywords\": analysis.get(\"keywords\", []),\n",
    "        \"field\": analysis.get(\"field\"),\n",
    "        \"summary_korean\": summary,\n",
    "        \"embedding_dim\": len(embedding),\n",
    "        \"embedding_preview\": embedding[:5],\n",
    "    }\n",
    "\n",
    "\n",
    "# Test the analysis pipeline\n",
    "test_article = {\n",
    "    \"title\": \"Attention Is All You Need\",\n",
    "    \"abstract\": \"\"\"\n",
    "    The dominant sequence transduction models are based on complex recurrent or\n",
    "    convolutional neural networks in an encoder-decoder configuration. The best\n",
    "    performing models also connect the encoder and decoder through an attention\n",
    "    mechanism. We propose a new simple network architecture, the Transformer,\n",
    "    based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
    "    entirely.\n",
    "    \"\"\",\n",
    "}\n",
    "\n",
    "result = await analyze_research_article(test_article[\"title\"], test_article[\"abstract\"])\n",
    "\n",
    "print(\"Research Article Analysis Result:\")\n",
    "print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "이 노트북에서 다룬 내용:\n",
    "\n",
    "1. ✓ Basic chat completion (OpenAI & Claude)\n",
    "2. ✓ JSON response format\n",
    "3. ✓ Korean summarization\n",
    "4. ✓ Embedding generation and similarity\n",
    "5. ✓ Async operations\n",
    "6. ✓ Batch processing\n",
    "7. ✓ Temperature comparison\n",
    "8. ✓ Error handling\n",
    "9. ✓ Complete research article analysis pipeline\n",
    "\n",
    "LLM 클라이언트가 정상적으로 작동하며, 다양한 사용 사례에 활용할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
