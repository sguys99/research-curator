{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Client test\n",
    "\n",
    "Day1 작업 중에 LLM 통합 관련 기능 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Setup complete\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from app.llm import LLMClient\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(project_root / \".env\")\n",
    "\n",
    "print(\"✓ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Basic Chat Completion\n",
    "\n",
    "기본적인 채팅 완성 기능 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Response:\n",
      "2024년에는 AI 분야에서 몇 가지 주요 트렌드가 주목받을 것으로 예상됩니다. 다음은 그 중 5가지입니다:\n",
      "\n",
      "1. **생성 AI의 발전과 응용 확대**: 생성형 AI는 텍스트, 이미지, 음악, 비디오 등 다양한 콘텐츠를 생성하는 데 사용됩니다. 이러한 기술의 정확성과 창의성이 더욱 향상되면서 마케팅, 엔터테인먼트, 교육 등 여러 분야에서 활용이 늘어날 것입니다.\n",
      "\n",
      "2. **AI의 윤리 및 규제 강화**: AI 기술의 급속한 발전과 함께 개인 정보 보호, 편향성, 투명성 등의 윤리적 문제에 대한 논의가 계속될 것입니다. 이에 따라 정부와 기관들이 AI 기술에 대한 규제를 강화하고, AI 시스템의 신뢰성과 책임성을 높이기 위한 노력이 증가할 것입니다.\n",
      "\n",
      "3. **엣지 AI와 분산 컴퓨팅**: 엣지 컴퓨팅을 통해 데이터를 중앙 서버가 아닌 데이터 생성 지점에서 처리함으로써 실시간 처리와 데이터 프라이버시가 중요한 분야에서 AI의 채택이 증가할 것입니다. 특히 IoT 기기와의 결합이 주목받을 것입니다.\n",
      "\n",
      "4. **멀티모달 AI의 발전**: 텍스트, 이미지, 오디오 등 다양한 데이터를 동시에 처리하고 이해하는 멀티모달 AI 시스템이 더욱 발전할 것입니다.\n"
     ]
    }
   ],
   "source": [
    "# OpenAI 클라이언트 생성\n",
    "openai_client = LLMClient(provider=\"openai\", temperature=0.7)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI research assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"2024년 AI 분야 키 트렌드 5가지를 알려줘.\"},\n",
    "]\n",
    "\n",
    "response = openai_client.chat_completion(messages, max_tokens=300)\n",
    "print(\"OpenAI Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Claude Chat Completion\n",
    "\n",
    "Claude API를 사용한 채팅 완성을 테스트."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude Response:\n",
      "# 2024년 AI 분야 주요 트렌드 5가지\n",
      "\n",
      "## 1. **생성형 AI의 실용화 및 기업 도입 가속화**\n",
      "- ChatGPT, Claude 등 LLM의 기업 업무 통합\n",
      "- AI 코파일럿(Copilot) 도구들의 확산\n",
      "- 맞춤형 GPT 및 엔터프라이즈 솔루션 증가\n",
      "\n",
      "## 2. **멀티모달 AI의 부상**\n",
      "- 텍스트, 이미지, 음성, 영상을 통합 처리하는 AI\n",
      "- GPT-4V, Gemini 등 멀티모달 모델 상용화\n",
      "- 더 자연스러운 인간-AI 상호작용 구현\n",
      "\n",
      "## 3. **AI 규제 및 윤리 프레임워크 강화**\n",
      "- EU AI Act 등 글로벌 AI 규제 본격화\n",
      "- AI 안전성, 투명성, 책임성에 대한 관심 증가\n"
     ]
    }
   ],
   "source": [
    "# Claude 클라이언트 생성 (API 키가 설정된 경우에만)\n",
    "try:\n",
    "    claude_client = LLMClient(provider=\"claude\", temperature=0.7)\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": \"2024년 AI 분야 키 트렌드 5가지를 알려줘.\"}]\n",
    "\n",
    "    response = claude_client.chat_completion(messages, max_tokens=300)\n",
    "    print(\"Claude Response:\")\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"Claude API not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. JSON Response Format\n",
    "\n",
    "JSON 형식의 응답을 요청하고 파싱."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Response:\n",
      "{\n",
      "    \"category\": \"news\",\n",
      "    \"importance_score\": 0.9,\n",
      "    \"keywords\": [\"GPT-5\", \"human-level performance\", \"complex reasoning\"],\n",
      "    \"summary\": \"GPT-5 demonstrates human-level capabilities in complex reasoning tasks.\"\n",
      "}\n",
      "\n",
      "Parsed Data:\n",
      "{\n",
      "  \"category\": \"news\",\n",
      "  \"importance_score\": 0.9,\n",
      "  \"keywords\": [\n",
      "    \"GPT-5\",\n",
      "    \"human-level performance\",\n",
      "    \"complex reasoning\"\n",
      "  ],\n",
      "  \"summary\": \"GPT-5 demonstrates human-level capabilities in complex reasoning tasks.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "client = LLMClient(provider=\"openai\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in JSON format.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"\n",
    "        기사 타이틀을 분석하고 다음 기준으로 JSOM 포맷으로 정리해줘.:\n",
    "        - category: one of (paper, news, report)\n",
    "        - importance_score: 0.0 to 1.0\n",
    "        - keywords: list of 3-5 relevant keywords\n",
    "        - summary: brief one-line summary\n",
    "\n",
    "        Title: \"GPT-5 Achieves Human-Level Performance on Complex Reasoning Tasks\"\n",
    "        \"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "response = client.chat_completion(messages, response_format=\"json\", max_tokens=300)\n",
    "print(\"JSON Response:\")\n",
    "print(response)\n",
    "\n",
    "# Parse JSON\n",
    "data = json.loads(response)\n",
    "print(\"\\nParsed Data:\")\n",
    "print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Article Summarization (Korean)\n",
    "\n",
    "논문을 한국어로 요약하는 기능을 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean Summary:\n",
      "MIT 연구진이 새로운 신경망 구조를 개발했습니다. 이 구조는 트랜스포머의 장점과 합성곱 신경망의 효율성을 결합한 하이브리드 모델로, TransConv라고 불립니다. 이 모델은 이미지 분류 작업에서 최첨단 성능을 발휘하면서도 전통적인 트랜스포머 모델보다 40% 적은 계산 자원을 요구합니다. 핵심 혁신은 입력 특성에 따라 지역 및 전역 특징 처리를 선택적으로 수행하는 주의 메커니즘에 있습니다.\n"
     ]
    }
   ],
   "source": [
    "client = LLMClient(provider=\"openai\", temperature=0.5)\n",
    "\n",
    "article_text = \"\"\"\n",
    "Researchers at MIT have developed a new neural network architecture\n",
    "that combines the benefits of transformers with the efficiency of\n",
    "convolutional neural networks. The hybrid model, dubbed TransConv,\n",
    "achieves state-of-the-art results on image classification tasks while\n",
    "requiring 40% less computational resources than traditional transformer\n",
    "models. The key innovation lies in the selective attention mechanism\n",
    "that adaptively chooses between local and global feature processing\n",
    "based on the input characteristics.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI research expert. Summarize articles in Korean.\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"다음 논문 내용을 한국어로 3-4문장으로 요약해주세요:\\n\\n{article_text}\",\n",
    "    },\n",
    "]\n",
    "\n",
    "summary = client.chat_completion(messages, max_tokens=500)\n",
    "print(\"Korean Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Embedding Generation\n",
    "\n",
    "텍스트 임베딩을 생성하고 유사도를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4 embeddings\n",
      "Embedding dimension: 1536\n",
      "First embedding (first 5 values): [-0.030603239312767982, -0.06797607243061066, 0.004770813975483179, -0.015896877273917198, 0.019433407112956047]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "client = LLMClient(provider=\"openai\")\n",
    "\n",
    "# 여러 텍스트의 임베딩 생성\n",
    "texts = [\n",
    "    \"Transformer architecture in deep learning\",\n",
    "    \"Attention mechanism for neural networks\",\n",
    "    \"Reinforcement learning for robotics\",\n",
    "    \"Computer vision using CNNs\",\n",
    "]\n",
    "\n",
    "embeddings = [client.generate_embedding(text) for text in texts]\n",
    "\n",
    "print(f\"Generated {len(embeddings)} embeddings\")\n",
    "print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
    "print(f\"First embedding (first 5 values): {embeddings[0][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine Similarities:\n",
      "'Transformer architecture in deep learning' vs 'Attention mechanism for neural networks': 0.4853\n",
      "'Transformer architecture in deep learning' vs 'Reinforcement learning for robotics': 0.3040\n",
      "'Transformer architecture in deep learning' vs 'Computer vision using CNNs': 0.4224\n",
      "'Attention mechanism for neural networks' vs 'Reinforcement learning for robotics': 0.3288\n",
      "'Attention mechanism for neural networks' vs 'Computer vision using CNNs': 0.3612\n",
      "'Reinforcement learning for robotics' vs 'Computer vision using CNNs': 0.3521\n"
     ]
    }
   ],
   "source": [
    "# 코사인 유사도 계산\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "\n",
    "print(\"\\nCosine Similarities:\")\n",
    "for i, text_i in enumerate(texts):\n",
    "    for j, text_j in enumerate(texts):\n",
    "        if i < j:\n",
    "            similarity = cosine_similarity(embeddings[i], embeddings[j])\n",
    "            print(f\"'{text_i}' vs '{text_j}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Async Operations\n",
    "\n",
    "비동기 작업을 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Async Chat Response:\n",
      "머신러닝은 컴퓨터가 명시적인 프로그래밍 없이도 데이터로부터 학습하고 예측하거나 결정을 내릴 수 있도록 하는 인공지능의 한 분야입니다.\n",
      "\n",
      "Async Embedding dimension: 1536\n",
      "First 5 values: [-0.011855199001729488, -0.02571532316505909, 0.03698211908340454, -0.026695990934967995, -0.014067153446376324]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def test_async_operations():\n",
    "    client = LLMClient(provider=\"openai\")\n",
    "\n",
    "    # Async chat completion\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"머신러닝을 한문장으로 정의한다면?\"},\n",
    "    ]\n",
    "\n",
    "    response = await client.achat_completion(messages, max_tokens=100)\n",
    "    print(\"Async Chat Response:\")\n",
    "    print(response)\n",
    "    print()\n",
    "\n",
    "    # Async embedding\n",
    "    embedding = await client.agenerate_embedding(\"Deep learning for NLP\")\n",
    "    print(f\"Async Embedding dimension: {len(embedding)}\")\n",
    "    print(f\"First 5 values: {embedding[:5]}\")\n",
    "\n",
    "\n",
    "# Run async function\n",
    "await test_async_operations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Batch Processing\n",
    "\n",
    "여러 문서를 동시에 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Processing Results:\n",
      "\n",
      "1. Original: GPT-5 shows remarkable improvements in reasoning capabilities.\n",
      "   Summary: GPT-5는 추론 능력에서 눈에 띄는 향상을 보였습니다. 이 모델은 복잡한 문제를 해결하고 논리적인 결론을 도출하는 데 있어 이전 버전보다 더 뛰어난 성능을 발휘합니다.\n",
      "\n",
      "2. Original: New transformer architecture reduces computational costs by 40%.\n",
      "   Summary: 새로운 트랜스포머 아키텍처는 계산 비용을 40% 절감한다. 이로 인해 더 효율적인 데이터 처리 및 모델 학습이 가능해졌다.\n",
      "\n",
      "3. Original: Reinforcement learning breakthrough enables better robot control.\n",
      "   Summary: 강화 학습의 돌파구가 로봇 제어를 더욱 향상시켰습니다. 이 기술은 로봇이 더 복잡한 작업을 효율적으로 수행할 수 있도록 지원합니다.\n"
     ]
    }
   ],
   "source": [
    "async def process_articles_batch(articles):\n",
    "    \"\"\"여러 논문을 동시에 요약\"\"\"\n",
    "    client = LLMClient(provider=\"openai\", temperature=0.5)\n",
    "\n",
    "    async def summarize_one(article):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Summarize the article in Korean in 2 sentences.\"},\n",
    "            {\"role\": \"user\", \"content\": article},\n",
    "        ]\n",
    "        return await client.achat_completion(messages, max_tokens=200)\n",
    "\n",
    "    # 모든 요약을 동시에 실행\n",
    "    summaries = await asyncio.gather(*[summarize_one(article) for article in articles])\n",
    "    return summaries\n",
    "\n",
    "\n",
    "# Test batch processing\n",
    "articles = [\n",
    "    \"GPT-5 shows remarkable improvements in reasoning capabilities.\",\n",
    "    \"New transformer architecture reduces computational costs by 40%.\",\n",
    "    \"Reinforcement learning breakthrough enables better robot control.\",\n",
    "]\n",
    "\n",
    "summaries = await process_articles_batch(articles)\n",
    "\n",
    "print(\"Batch Processing Results:\")\n",
    "for i, (article, summary) in enumerate(zip(articles, summaries, strict=False), 1):\n",
    "    print(f\"\\n{i}. Original: {article}\")\n",
    "    print(f\"   Summary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Temperature Comparison\n",
    "\n",
    "다양한 temperature 값에 따른 응답 차이를 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature Comparison:\n",
      "Prompt: Explain neural networks in one sentence.\n",
      "\n",
      "Temperature 0.0:\n",
      "  Neural networks are computational models inspired by the human brain, consisting of interconnected layers of nodes (neurons) that process and learn from data to perform tasks such as classification, regression, and pattern recognition.\n",
      "\n",
      "Temperature 0.5:\n",
      "  Neural networks are computational models inspired by the human brain, consisting of interconnected layers of nodes (or \"neurons\") that process and learn from data through weighted connections and activation functions.\n",
      "\n",
      "Temperature 1.0:\n",
      "  Neural networks are computational models inspired by the human brain, consisting of interconnected layers of nodes (neurons) that process input data to recognize patterns and make predictions or decisions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperatures = [0.0, 0.5, 1.0]\n",
    "prompt = \"Explain neural networks in one sentence.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "print(\"Temperature Comparison:\")\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "for temp in temperatures:\n",
    "    client = LLMClient(provider=\"openai\", temperature=temp)\n",
    "    response = client.chat_completion(messages, max_tokens=100)\n",
    "    print(f\"Temperature {temp}:\")\n",
    "    print(f\"  {response}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Error Handling\n",
    "\n",
    "에러 처리를 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Caught expected error for invalid provider: Unsupported provider: invalid\n",
      "Error with large max_tokens: LLM completion failed: litellm.BadRequestError: OpenAIException - max_tokens is too large: 100000. This model supports at most 16384 completion tokens, whereas you provided 100000.\n"
     ]
    }
   ],
   "source": [
    "# Test invalid provider\n",
    "try:\n",
    "    invalid_client = LLMClient(provider=\"invalid\")\n",
    "except ValueError as e:\n",
    "    print(f\"✓ Caught expected error for invalid provider: {e}\")\n",
    "\n",
    "# Test with very large max_tokens (should be handled by API)\n",
    "try:\n",
    "    client = LLMClient(provider=\"openai\")\n",
    "    messages = [{\"role\": \"user\", \"content\": \"Hello\"}]\n",
    "    response = client.chat_completion(messages, max_tokens=100000)\n",
    "    print(\"\\nResponse with large max_tokens received (API handled it)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with large max_tokens: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use Case: Research Article Analysis\n",
    "\n",
    "실제 사용 사례: 연구 논문 분석 파이프라인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Article Analysis Result:\n",
      "{\n",
      "  \"title\": \"Attention Is All You Need\",\n",
      "  \"category\": \"paper\",\n",
      "  \"importance_score\": 0.95,\n",
      "  \"keywords\": [\n",
      "    \"Transformer\",\n",
      "    \"attention mechanism\",\n",
      "    \"sequence transduction\",\n",
      "    \"encoder-decoder\",\n",
      "    \"neural networks\"\n",
      "  ],\n",
      "  \"field\": \"NLP\",\n",
      "  \"summary_korean\": \"\\\"Attention Is All You Need\\\" 논문은 기존의 복잡한 순환 신경망이나 합성곱 신경망 기반의 인코더-디코더 모델 대신, 주의 메커니즘만을 활용한 새로운 간단한 네트워크 아키텍처인 트랜스포머를 제안합니다. 이 모델은 순환과 합성곱을 완전히 배제하고, 인코더와 디코더를 주의 메커니즘을 통해 연결하여 성능을 향상시킵니다.\",\n",
      "  \"embedding_dim\": 1536,\n",
      "  \"embedding_preview\": [\n",
      "    0.020583556964993477,\n",
      "    -0.007367096375674009,\n",
      "    -0.016429178416728973,\n",
      "    0.009078700095415115,\n",
      "    0.02123717963695526\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "async def analyze_research_article(title, abstract):\n",
    "    \"\"\"연구 논문을 분석하고 메타데이터를 추출\"\"\"\n",
    "    client = LLMClient(provider=\"openai\", temperature=0.3)\n",
    "\n",
    "    # 1. 카테고리 분류 및 중요도 평가\n",
    "    analysis_messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an AI research expert. Analyze research articles and respond in JSON.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "            Analyze this research article and return JSON with:\n",
    "            - category: one of (paper, news, report)\n",
    "            - importance_score: 0.0 to 1.0\n",
    "            - keywords: list of 5 keywords\n",
    "            - field: research field (e.g., \"Computer Vision\", \"NLP\", \"Robotics\")\n",
    "\n",
    "            Title: {title}\n",
    "            Abstract: {abstract}\n",
    "            \"\"\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # 2. 한국어 요약\n",
    "    summary_content = (\n",
    "        f\"다음 논문을 한국어로 3-4문장으로 요약해주세요:\\n\\n\" f\"Title: {title}\\n\\nAbstract: {abstract}\"\n",
    "    )\n",
    "    summary_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an AI research expert. Summarize in Korean.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": summary_content,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # 3. 임베딩 생성\n",
    "    embedding_text = f\"{title} {abstract}\"\n",
    "\n",
    "    # 모두 비동기로 동시 실행\n",
    "    analysis_task = client.achat_completion(analysis_messages, response_format=\"json\", max_tokens=300)\n",
    "    summary_task = client.achat_completion(summary_messages, max_tokens=500)\n",
    "    embedding_task = client.agenerate_embedding(embedding_text)\n",
    "\n",
    "    analysis_response, summary, embedding = await asyncio.gather(\n",
    "        analysis_task, summary_task, embedding_task\n",
    "    )\n",
    "\n",
    "    # 결과 파싱\n",
    "    analysis = json.loads(analysis_response)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"category\": analysis.get(\"category\"),\n",
    "        \"importance_score\": analysis.get(\"importance_score\"),\n",
    "        \"keywords\": analysis.get(\"keywords\", []),\n",
    "        \"field\": analysis.get(\"field\"),\n",
    "        \"summary_korean\": summary,\n",
    "        \"embedding_dim\": len(embedding),\n",
    "        \"embedding_preview\": embedding[:5],\n",
    "    }\n",
    "\n",
    "\n",
    "# Test the analysis pipeline\n",
    "test_article = {\n",
    "    \"title\": \"Attention Is All You Need\",\n",
    "    \"abstract\": \"\"\"\n",
    "    The dominant sequence transduction models are based on complex recurrent or\n",
    "    convolutional neural networks in an encoder-decoder configuration. The best\n",
    "    performing models also connect the encoder and decoder through an attention\n",
    "    mechanism. We propose a new simple network architecture, the Transformer,\n",
    "    based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
    "    entirely.\n",
    "    \"\"\",\n",
    "}\n",
    "\n",
    "result = await analyze_research_article(test_article[\"title\"], test_article[\"abstract\"])\n",
    "\n",
    "print(\"Research Article Analysis Result:\")\n",
    "print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Streaming Response\n",
    "\n",
    "스트리밍 방식으로 실시간 응답을 받는 기능을 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming Response:\n",
      "--------------------------------------------------\n",
      "AI 기술의 미래는 더욱 정교하고 인간과 비슷한 수준의 이해 및 의사소통 능력을 갖춘 시스템의 개발로 이어질 것입니다. 이러한 기술은 의료, 교육, 교통 등 다양한 분야에서 혁신을 가져와 생산성을 높이고 삶의 질을 향상시킬 것입니다. 그러나 AI의 발전은 윤리적 문제와 개인정보 보호 등의 새로운 도전을 동반할 것이며, 이에 대한 사회적 합의와 법적 규제가 필요할 것입니다.\n",
      "--------------------------------------------------\n",
      "\n",
      "Full response length: 209 characters\n"
     ]
    }
   ],
   "source": [
    "# Test streaming response\n",
    "client = LLMClient(provider=\"openai\", temperature=0.7)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"AI 기술의 미래에 대해 3문장으로 설명해줘.\"},\n",
    "]\n",
    "\n",
    "print(\"Streaming Response:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# stream=True로 호출하면 LiteLLM generator가 반환됨\n",
    "response_stream = client.chat_completion(messages, max_tokens=200, stream=True)\n",
    "\n",
    "# 사용자가 직접 stream을 처리\n",
    "full_response = \"\"\n",
    "for chunk in response_stream:\n",
    "    # LiteLLM의 streaming chunk 구조에서 content 추출\n",
    "    if chunk.choices[0].delta.content:\n",
    "        text = chunk.choices[0].delta.content\n",
    "        print(text, end=\"\", flush=True)\n",
    "        full_response += text\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(f\"\\nFull response length: {len(full_response)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Async Streaming Response:\n",
      "--------------------------------------------------\n",
      "머신러닝과 딥러닝은 둘 다 인공지능의 하위 분야로, 데이터로부터 학습하여 작업을 수행하는 기술입니다. 하지만 둘 사이에는 몇 가지 중요한 차이점이 있습니다.\n",
      "\n",
      "1. **구조**:\n",
      "   - **머신러닝**: 주로 선형 회귀, 로지스틱 회귀, 결정 트리, 서포트 벡터 머신(SVM), K-평균 군집화 등 다양한 알고리즘을 포함합니다. 데이터의 특성을 수동으로 추출하고 선택하는 과정을 많이 요구합니다.\n",
      "   - **딥러닝**: 인공신경망을 기반으로 한 머신러닝의 하위 분야입니다. 특히 여러 층을 가진 심층 신경망(Deep Neural Networks)을 사용하여 데이터로부터 자동으로 특징을 학습합니다.\n",
      "\n",
      "2. **데이터 요구량**:\n",
      "   - **머신러닝**: 상대\n",
      "--------------------------------------------------\n",
      "\n",
      "Full response length: 371 characters\n"
     ]
    }
   ],
   "source": [
    "# Test async streaming response\n",
    "async def test_async_streaming():\n",
    "    client = LLMClient(provider=\"openai\", temperature=0.7)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"머신러닝과 딥러닝의 차이를 간단히 설명해줘.\"},\n",
    "    ]\n",
    "\n",
    "    print(\"\\nAsync Streaming Response:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # stream=True로 호출하면 LiteLLM async generator가 반환됨\n",
    "    response_stream = await client.achat_completion(messages, max_tokens=200, stream=True)\n",
    "\n",
    "    # 사용자가 직접 async stream을 처리\n",
    "    full_response = \"\"\n",
    "    async for chunk in response_stream:\n",
    "        # LiteLLM의 streaming chunk 구조에서 content 추출\n",
    "        if chunk.choices[0].delta.content:\n",
    "            text = chunk.choices[0].delta.content\n",
    "            print(text, end=\"\", flush=True)\n",
    "            full_response += text\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 50)\n",
    "    print(f\"\\nFull response length: {len(full_response)} characters\")\n",
    "\n",
    "\n",
    "await test_async_streaming()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "이 노트북에서 다룬 내용:\n",
    "\n",
    "1. ✓ Basic chat completion (OpenAI & Claude)\n",
    "2. ✓ JSON response format\n",
    "3. ✓ Korean summarization\n",
    "4. ✓ Embedding generation and similarity\n",
    "5. ✓ Async operations\n",
    "6. ✓ Batch processing\n",
    "7. ✓ Temperature comparison\n",
    "8. ✓ Error handling\n",
    "9. ✓ Complete research article analysis pipeline\n",
    "10. ✓ Research article analysis pipeline\n",
    "11. ✓ Streaming response (sync & async)\n",
    "\n",
    "LLM 클라이언트가 정상적으로 작동하며, 다양한 사용 사례에 활용할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
