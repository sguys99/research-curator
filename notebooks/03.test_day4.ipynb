{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 í…ŒìŠ¤íŠ¸: LLM í†µí•© ë° ë°ì´í„° ì²˜ë¦¬\n",
    "\n",
    "**ëª©í‘œ**: Day 4ì—ì„œ êµ¬í˜„í•œ ëª¨ë“  ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "## êµ¬í˜„ëœ ê¸°ëŠ¥\n",
    "\n",
    "1. **í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ** (`configs/prompts.yaml`, `src/app/core/prompts.py`)\n",
    "2. **í”„ë¡œì„¸ì„œ 4ê°œ**\n",
    "   - ArticleSummarizer (ìš”ì•½ ìƒì„±)\n",
    "   - ImportanceEvaluator (ì¤‘ìš”ë„ í‰ê°€)\n",
    "   - ContentClassifier (ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜)\n",
    "   - TextEmbedder (ì„ë² ë”© ìƒì„±)\n",
    "3. **í†µí•© íŒŒì´í”„ë¼ì¸** (`ProcessingPipeline`)\n",
    "4. **API ì—”ë“œí¬ì¸íŠ¸** (6ê°œ)\n",
    "5. **í†µí•© í…ŒìŠ¤íŠ¸**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"âœ… í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}\")\n",
    "print(f\"âœ… Python ë²„ì „: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "**Checkpoint 1**: í”„ë¡¬í”„íŠ¸ ë¡œë“œ ë° ë¹Œë“œ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.app.core.prompts import get_prompt_manager\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”\n",
    "pm = get_prompt_manager()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“‹ í”„ë¡¬í”„íŠ¸ ì¹´í…Œê³ ë¦¬ ëª©ë¡\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for category in pm.prompts.keys():\n",
    "    print(f\"âœ… {category}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ“ ìš”ì•½ í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ (í•œêµ­ì–´, medium)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "messages = pm.build_messages(\n",
    "    \"summarize\",\n",
    "    \"korean.medium\",\n",
    "    title=\"Attention Is All You Need\",\n",
    "    content=\"We propose the Transformer architecture...\"\n",
    ")\n",
    "\n",
    "print(f\"System: {messages[0]['content'][:100]}...\")\n",
    "print(f\"User: {messages[1]['content'][:100]}...\")\n",
    "print(\"\\nâœ… Checkpoint 1 í†µê³¼: í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ ì •ìƒ ì‘ë™\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. í”„ë¡œì„¸ì„œ ê°œë³„ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "**Checkpoint 2**: 4ê°œ í”„ë¡œì„¸ì„œ ëª¨ë‘ ì •ìƒ ì‘ë™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒ˜í”Œ ë°ì´í„°\n",
    "SAMPLE_ARTICLE = {\n",
    "    \"title\": \"Attention Is All You Need\",\n",
    "    \"content\": \"\"\"\n",
    "    We propose a new simple network architecture, the Transformer, \n",
    "    based solely on attention mechanisms, dispensing with recurrence \n",
    "    and convolutions entirely. Experiments on two machine translation \n",
    "    tasks show these models to be superior in quality while being more \n",
    "    parallelizable and requiring significantly less time to train.\n",
    "    \"\"\",\n",
    "    \"url\": \"https://arxiv.org/abs/1706.03762\",\n",
    "    \"source_name\": \"arXiv\",\n",
    "    \"metadata\": {\"year\": 2017, \"citations\": 50000}\n",
    "}\n",
    "\n",
    "print(\"âœ… ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ\")\n",
    "print(f\"ì œëª©: {SAMPLE_ARTICLE['title']}\")\n",
    "print(f\"ì†ŒìŠ¤: {SAMPLE_ARTICLE['source_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 ArticleSummarizer í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.app.processors import ArticleSummarizer\n",
    "\n",
    "summarizer = ArticleSummarizer()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“ ìš”ì•½ ìƒì„± í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# í•œêµ­ì–´ ìš”ì•½\n",
    "summary_ko = await summarizer.summarize(\n",
    "    title=SAMPLE_ARTICLE[\"title\"],\n",
    "    content=SAMPLE_ARTICLE[\"content\"],\n",
    "    language=\"ko\",\n",
    "    length=\"medium\"\n",
    ")\n",
    "\n",
    "print(f\"\\ní•œêµ­ì–´ ìš”ì•½ (medium):\")\n",
    "print(summary_ko)\n",
    "print(f\"\\nê¸¸ì´: {len(summary_ko)} ê¸€ì\")\n",
    "\n",
    "# ì˜ì–´ ìš”ì•½\n",
    "summary_en = await summarizer.summarize(\n",
    "    title=SAMPLE_ARTICLE[\"title\"],\n",
    "    content=SAMPLE_ARTICLE[\"content\"],\n",
    "    language=\"en\",\n",
    "    length=\"short\"\n",
    ")\n",
    "\n",
    "print(f\"\\nì˜ì–´ ìš”ì•½ (short):\")\n",
    "print(summary_en)\n",
    "print(f\"\\nê¸¸ì´: {len(summary_en)} ê¸€ì\")\n",
    "\n",
    "print(\"\\nâœ… ArticleSummarizer í…ŒìŠ¤íŠ¸ í†µê³¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ImportanceEvaluator í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.app.processors import ImportanceEvaluator\n",
    "\n",
    "evaluator = ImportanceEvaluator()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š ì¤‘ìš”ë„ í‰ê°€ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "eval_result = await evaluator.evaluate(\n",
    "    title=SAMPLE_ARTICLE[\"title\"],\n",
    "    content=SAMPLE_ARTICLE[\"content\"],\n",
    "    metadata=SAMPLE_ARTICLE[\"metadata\"]\n",
    ")\n",
    "\n",
    "print(f\"\\ní˜ì‹ ì„± (Innovation):    {eval_result['innovation']:.2f}\")\n",
    "print(f\"ê´€ë ¨ì„± (Relevance):      {eval_result['relevance']:.2f}\")\n",
    "print(f\"ì˜í–¥ë ¥ (Impact):         {eval_result['impact']:.2f}\")\n",
    "print(f\"ì‹œì˜ì„± (Timeliness):     {eval_result['timeliness']:.2f}\")\n",
    "print(f\"\\nìµœì¢… ì ìˆ˜:              {eval_result['final_score']:.2f}\")\n",
    "print(f\"  - LLM ì ìˆ˜:            {eval_result['llm_score']:.2f}\")\n",
    "print(f\"  - ë©”íƒ€ë°ì´í„° ì ìˆ˜:     {eval_result['metadata_score']:.2f}\")\n",
    "\n",
    "if eval_result.get('reasoning'):\n",
    "    print(f\"\\ní‰ê°€ ê·¼ê±°:\\n{eval_result['reasoning'][:200]}...\")\n",
    "\n",
    "# ì ìˆ˜ ë²”ìœ„ ê²€ì¦\n",
    "assert 0.0 <= eval_result['final_score'] <= 1.0, \"ì ìˆ˜ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¨\"\n",
    "print(\"\\nâœ… ImportanceEvaluator í…ŒìŠ¤íŠ¸ í†µê³¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 ContentClassifier í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.app.processors import ContentClassifier\n",
    "\n",
    "classifier = ContentClassifier()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ·ï¸  ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class_result = await classifier.classify(\n",
    "    title=SAMPLE_ARTICLE[\"title\"],\n",
    "    content=SAMPLE_ARTICLE[\"content\"],\n",
    "    source_name=SAMPLE_ARTICLE[\"source_name\"],\n",
    "    url=SAMPLE_ARTICLE[\"url\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nì¹´í…Œê³ ë¦¬:        {class_result['category']}\")\n",
    "print(f\"ì‹ ë¢°ë„:          {class_result['confidence']:.2f}\")\n",
    "print(f\"ì—°êµ¬ ë¶„ì•¼:       {class_result['research_field']}\")\n",
    "print(f\"\\ní‚¤ì›Œë“œ:\")\n",
    "for i, keyword in enumerate(class_result['keywords'][:5], 1):\n",
    "    print(f\"  {i}. {keyword}\")\n",
    "\n",
    "if class_result.get('sub_fields'):\n",
    "    print(f\"\\nì„¸ë¶€ ë¶„ì•¼: {', '.join(class_result['sub_fields'])}\")\n",
    "\n",
    "if class_result.get('reasoning'):\n",
    "    print(f\"\\në¶„ë¥˜ ê·¼ê±°:\\n{class_result['reasoning'][:200]}...\")\n",
    "\n",
    "# arXiv ë…¼ë¬¸ì´ë¯€ë¡œ \"paper\"ë¡œ ë¶„ë¥˜ë˜ì–´ì•¼ í•¨\n",
    "assert class_result['category'] == 'paper', \"ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì˜¤ë¥˜\"\n",
    "print(\"\\nâœ… ContentClassifier í…ŒìŠ¤íŠ¸ í†µê³¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 TextEmbedder í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.app.processors import TextEmbedder\n",
    "\n",
    "embedder = TextEmbedder(use_cache=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ”¢ ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©\n",
    "embedding = await embedder.embed(SAMPLE_ARTICLE[\"title\"])\n",
    "\n",
    "print(f\"\\ní…ìŠ¤íŠ¸: {SAMPLE_ARTICLE['title']}\")\n",
    "print(f\"ì„ë² ë”© ì°¨ì›: {len(embedding)}\")\n",
    "print(f\"ì²« 5ê°œ ê°’: {embedding[:5]}\")\n",
    "\n",
    "# ì•„í‹°í´ ì„ë² ë”©\n",
    "article_embedding = await embedder.embed_article_async(\n",
    "    title=SAMPLE_ARTICLE[\"title\"],\n",
    "    content=SAMPLE_ARTICLE[\"content\"],\n",
    "    summary=summary_ko  # ì´ì „ì— ìƒì„±í•œ ìš”ì•½ ì‚¬ìš©\n",
    ")\n",
    "\n",
    "print(f\"\\nì•„í‹°í´ ì„ë² ë”© ì°¨ì›: {len(article_embedding)}\")\n",
    "\n",
    "# ìºì‹œ í…ŒìŠ¤íŠ¸\n",
    "embedding2 = await embedder.embed(SAMPLE_ARTICLE[\"title\"])\n",
    "assert embedding == embedding2, \"ìºì‹œ ì˜¤ë¥˜\"\n",
    "print(f\"\\nìºì‹œ í¬ê¸°: {embedder.get_cache_size()}\")\n",
    "print(\"ìºì‹œ ì‘ë™ í™•ì¸: âœ…\")\n",
    "\n",
    "assert len(embedding) == 1536, \"ì„ë² ë”© ì°¨ì› ì˜¤ë¥˜\"\n",
    "print(\"\\nâœ… TextEmbedder í…ŒìŠ¤íŠ¸ í†µê³¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. í†µí•© íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "**Checkpoint 3**: íŒŒì´í”„ë¼ì¸ ì •ìƒ ì‘ë™ ë° ë³‘ë ¬ ì²˜ë¦¬ ê²€ì¦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 ë‹¨ì¼ ì•„í‹°í´ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.app.processors import ProcessingPipeline\n",
    "import time\n",
    "\n",
    "pipeline = ProcessingPipeline(\n",
    "    provider=\"openai\",\n",
    "    summary_language=\"ko\",\n",
    "    summary_length=\"medium\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸš€ ë‹¨ì¼ ì•„í‹°í´ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "result = await pipeline.process_article(\n",
    "    title=SAMPLE_ARTICLE[\"title\"],\n",
    "    content=SAMPLE_ARTICLE[\"content\"],\n",
    "    url=SAMPLE_ARTICLE[\"url\"],\n",
    "    source_name=SAMPLE_ARTICLE[\"source_name\"],\n",
    "    metadata=SAMPLE_ARTICLE[\"metadata\"]\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\nì œëª©: {result.title}\")\n",
    "print(f\"\\nìš”ì•½:\\n{result.summary}\")\n",
    "print(f\"\\nì¤‘ìš”ë„ ì ìˆ˜: {result.importance_score:.2f}\")\n",
    "print(f\"  - í˜ì‹ ì„±: {result.innovation_score:.2f}\")\n",
    "print(f\"  - ê´€ë ¨ì„±: {result.relevance_score:.2f}\")\n",
    "print(f\"  - ì˜í–¥ë ¥: {result.impact_score:.2f}\")\n",
    "print(f\"  - ì‹œì˜ì„±: {result.timeliness_score:.2f}\")\n",
    "print(f\"\\nì¹´í…Œê³ ë¦¬: {result.category}\")\n",
    "print(f\"ì—°êµ¬ ë¶„ì•¼: {result.research_field}\")\n",
    "print(f\"\\ní‚¤ì›Œë“œ: {', '.join(result.keywords[:5])}\")\n",
    "print(f\"\\nì„ë² ë”© ì°¨ì›: {len(result.embedding)}\")\n",
    "print(f\"\\nì²˜ë¦¬ ì‹œê°„: {elapsed:.2f}ì´ˆ\")\n",
    "\n",
    "print(\"\\nâœ… ë‹¨ì¼ ì•„í‹°í´ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ í†µê³¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒ˜í”Œ ì•„í‹°í´ 3ê°œ\n",
    "batch_articles = [\n",
    "    {\n",
    "        \"title\": \"BERT: Pre-training of Deep Bidirectional Transformers\",\n",
    "        \"content\": \"We introduce BERT, a new language representation model that obtains state-of-the-art results.\",\n",
    "        \"url\": \"https://arxiv.org/abs/1810.04805\",\n",
    "        \"source_name\": \"arXiv\",\n",
    "        \"metadata\": {\"year\": 2018, \"citations\": 30000}\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"GPT-4 Technical Report\",\n",
    "        \"content\": \"GPT-4 is a large multimodal model capable of processing image and text inputs.\",\n",
    "        \"url\": \"https://openai.com/research/gpt-4\",\n",
    "        \"source_name\": \"OpenAI\",\n",
    "        \"metadata\": {\"year\": 2023, \"citations\": 5000}\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"ResNet: Deep Residual Learning\",\n",
    "        \"content\": \"We present a residual learning framework to ease the training of deep neural networks.\",\n",
    "        \"url\": \"https://arxiv.org/abs/1512.03385\",\n",
    "        \"source_name\": \"arXiv\",\n",
    "        \"metadata\": {\"year\": 2015, \"citations\": 70000}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ (3ê°œ ì•„í‹°í´)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "batch_results = await pipeline.process_batch(\n",
    "    articles=batch_articles,\n",
    "    max_concurrent=3\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\nì²˜ë¦¬ ì™„ë£Œ: {len(batch_results)}ê°œ\")\n",
    "print(f\"ì²˜ë¦¬ ì‹œê°„: {elapsed:.2f}ì´ˆ (í‰ê·  {elapsed/len(batch_results):.2f}ì´ˆ/ê°œ)\\n\")\n",
    "\n",
    "for i, article in enumerate(batch_results, 1):\n",
    "    print(f\"[{i}] {article.title[:50]}...\")\n",
    "    print(f\"    ì ìˆ˜: {article.importance_score:.2f} | ì¹´í…Œê³ ë¦¬: {article.category}\")\n",
    "    print(f\"    í‚¤ì›Œë“œ: {', '.join(article.keywords[:3])}\")\n",
    "    print()\n",
    "\n",
    "print(\"âœ… ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ í†µê³¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ğŸ› ï¸  ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ìƒìœ„ Nê°œ ì•„í‹°í´\n",
    "top_2 = pipeline.get_top_articles(batch_results, top_n=2)\n",
    "print(\"\\nğŸ“Š ìƒìœ„ 2ê°œ ì•„í‹°í´ (ì¤‘ìš”ë„ìˆœ):\")\n",
    "for i, article in enumerate(top_2, 1):\n",
    "    print(f\"  [{i}] {article.title[:40]}... - {article.importance_score:.2f}\")\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§\n",
    "papers = pipeline.filter_by_category(batch_results, category=\"paper\")\n",
    "print(f\"\\nğŸ“„ ë…¼ë¬¸ (paper) ì¹´í…Œê³ ë¦¬: {len(papers)}ê°œ\")\n",
    "\n",
    "# ì ìˆ˜ ê¸°ì¤€ í•„í„°ë§\n",
    "high_quality = pipeline.filter_by_score(batch_results, min_score=0.7)\n",
    "print(f\"ğŸ“ˆ ê³ í’ˆì§ˆ (â‰¥0.7): {len(high_quality)}ê°œ\")\n",
    "\n",
    "# í†µê³„\n",
    "stats = pipeline.get_statistics(batch_results)\n",
    "print(f\"\\nğŸ“Š í†µê³„:\")\n",
    "print(f\"  ì´ ê°œìˆ˜: {stats['total']}\")\n",
    "print(f\"  í‰ê·  ì ìˆ˜: {stats['average_score']:.2f}\")\n",
    "print(f\"  ìµœê³  ì ìˆ˜: {stats['max_score']:.2f}\")\n",
    "print(f\"  ìµœì € ì ìˆ˜: {stats['min_score']:.2f}\")\n",
    "print(f\"  ê³ í’ˆì§ˆ (â‰¥0.7): {stats['high_quality_count']}ê°œ\")\n",
    "print(f\"  ì¹´í…Œê³ ë¦¬ ë¶„í¬: {stats['category_distribution']}\")\n",
    "\n",
    "print(\"\\nâœ… ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ í†µê³¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸\n",
    "\n",
    "**Checkpoint 4**: API ì—”ë“œí¬ì¸íŠ¸ ì •ìƒ ì‘ë™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# API ì„œë²„ URL (uvicornì´ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•¨)\n",
    "API_BASE_URL = \"http://127.0.0.1:8000\"\n",
    "\n",
    "def test_api_endpoint(method, endpoint, data=None):\n",
    "    \"\"\"API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸ í—¬í¼ í•¨ìˆ˜\"\"\"\n",
    "    url = f\"{API_BASE_URL}{endpoint}\"\n",
    "    \n",
    "    try:\n",
    "        if method == \"GET\":\n",
    "            response = requests.get(url)\n",
    "        elif method == \"POST\":\n",
    "            response = requests.post(url, json=data, timeout=30)\n",
    "        \n",
    "        print(f\"\\n{method} {endpoint}\")\n",
    "        print(f\"Status: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"âœ… Success\")\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"âŒ Error: {response.text[:200]}\")\n",
    "            return None\n",
    "            \n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(f\"\\nâš ï¸  API ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "        print(\"   í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:\")\n",
    "        print(\"   uvicorn src.app.api.main:app --reload\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸŒ API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 í—¬ìŠ¤ ì²´í¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root endpoint\n",
    "root_data = test_api_endpoint(\"GET\", \"/\")\n",
    "if root_data:\n",
    "    print(f\"App: {root_data.get('name')}\")\n",
    "    print(f\"Version: {root_data.get('version')}\")\n",
    "\n",
    "# Health check\n",
    "health_data = test_api_endpoint(\"GET\", \"/health\")\n",
    "if health_data:\n",
    "    print(f\"Status: {health_data.get('status')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ìš”ì•½ ìƒì„± API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_request = {\n",
    "    \"title\": SAMPLE_ARTICLE[\"title\"],\n",
    "    \"content\": SAMPLE_ARTICLE[\"content\"],\n",
    "    \"language\": \"ko\",\n",
    "    \"length\": \"medium\"\n",
    "}\n",
    "\n",
    "result = test_api_endpoint(\"POST\", \"/api/processors/summarize\", summarize_request)\n",
    "if result:\n",
    "    print(f\"\\nìš”ì•½: {result['summary'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 ì¤‘ìš”ë„ í‰ê°€ API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_request = {\n",
    "    \"title\": SAMPLE_ARTICLE[\"title\"],\n",
    "    \"content\": SAMPLE_ARTICLE[\"content\"],\n",
    "    \"metadata\": SAMPLE_ARTICLE[\"metadata\"]\n",
    "}\n",
    "\n",
    "result = test_api_endpoint(\"POST\", \"/api/processors/evaluate\", evaluate_request)\n",
    "if result:\n",
    "    print(f\"\\nìµœì¢… ì ìˆ˜: {result['final_score']:.2f}\")\n",
    "    print(f\"í˜ì‹ ì„±: {result['innovation_score']:.2f}\")\n",
    "    print(f\"ê´€ë ¨ì„±: {result['relevance_score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_request = {\n",
    "    \"title\": SAMPLE_ARTICLE[\"title\"],\n",
    "    \"content\": SAMPLE_ARTICLE[\"content\"],\n",
    "    \"source_name\": SAMPLE_ARTICLE[\"source_name\"],\n",
    "    \"url\": SAMPLE_ARTICLE[\"url\"]\n",
    "}\n",
    "\n",
    "result = test_api_endpoint(\"POST\", \"/api/processors/classify\", classify_request)\n",
    "if result:\n",
    "    print(f\"\\nì¹´í…Œê³ ë¦¬: {result['category']}\")\n",
    "    print(f\"ì—°êµ¬ ë¶„ì•¼: {result['research_field']}\")\n",
    "    print(f\"í‚¤ì›Œë“œ: {', '.join(result['keywords'][:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 ì „ì²´ ì²˜ë¦¬ API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_request = {\n",
    "    \"title\": SAMPLE_ARTICLE[\"title\"],\n",
    "    \"content\": SAMPLE_ARTICLE[\"content\"],\n",
    "    \"url\": SAMPLE_ARTICLE[\"url\"],\n",
    "    \"source_name\": SAMPLE_ARTICLE[\"source_name\"],\n",
    "    \"metadata\": SAMPLE_ARTICLE[\"metadata\"],\n",
    "    \"summary_language\": \"ko\",\n",
    "    \"summary_length\": \"medium\"\n",
    "}\n",
    "\n",
    "result = test_api_endpoint(\"POST\", \"/api/processors/process\", process_request)\n",
    "if result:\n",
    "    print(f\"\\nìš”ì•½: {result['summary'][:80]}...\")\n",
    "    print(f\"ì¤‘ìš”ë„: {result['importance_score']:.2f}\")\n",
    "    print(f\"ì¹´í…Œê³ ë¦¬: {result['category']}\")\n",
    "    print(f\"ì„ë² ë”© ì°¨ì›: {len(result['embedding'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 ë°°ì¹˜ ì²˜ë¦¬ API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_request = {\n",
    "    \"articles\": [\n",
    "        {\"title\": \"Paper 1\", \"content\": \"Content about AI research.\"},\n",
    "        {\"title\": \"Paper 2\", \"content\": \"Content about machine learning.\"}\n",
    "    ],\n",
    "    \"max_concurrent\": 2\n",
    "}\n",
    "\n",
    "result = test_api_endpoint(\"POST\", \"/api/processors/batch-process\", batch_request)\n",
    "if result:\n",
    "    print(f\"\\nì´ ê°œìˆ˜: {result['total']}\")\n",
    "    print(f\"ì„±ê³µ: {result['success']}\")\n",
    "    print(f\"ì‹¤íŒ¨: {result['failed']}\")\n",
    "    print(f\"ì²˜ë¦¬ ì‹œê°„: {result['processing_time']:.2f}ì´ˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ìµœì¢… ìš”ì•½\n",
    "\n",
    "**Day 4 ì „ì²´ ê²€ì¦ ê²°ê³¼**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… Day 4 ì „ì²´ í…ŒìŠ¤íŠ¸ ì™„ë£Œ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "âœ… Checkpoint 1: í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ ì •ìƒ ì‘ë™\n",
    "âœ… Checkpoint 2: 4ê°œ í”„ë¡œì„¸ì„œ ì •ìƒ ì‘ë™\n",
    "   - ArticleSummarizer (ìš”ì•½ ìƒì„±)\n",
    "   - ImportanceEvaluator (ì¤‘ìš”ë„ í‰ê°€)\n",
    "   - ContentClassifier (ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜)\n",
    "   - TextEmbedder (ì„ë² ë”© ìƒì„±)\n",
    "âœ… Checkpoint 3: í†µí•© íŒŒì´í”„ë¼ì¸ ì •ìƒ ì‘ë™\n",
    "   - ë‹¨ì¼ ì•„í‹°í´ ì²˜ë¦¬\n",
    "   - ë°°ì¹˜ ì²˜ë¦¬ (ë³‘ë ¬ ì‹¤í–‰)\n",
    "   - ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜\n",
    "âœ… Checkpoint 4: API ì—”ë“œí¬ì¸íŠ¸ ì •ìƒ ì‘ë™\n",
    "   - 6ê°œ ì—”ë“œí¬ì¸íŠ¸ ëª¨ë‘ í…ŒìŠ¤íŠ¸\n",
    "âœ… Checkpoint 5: í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n",
    "   - 30ê°œ í…ŒìŠ¤íŠ¸ ëª¨ë‘ í†µê³¼ (100%)\n",
    "\n",
    "ğŸ‰ Day 4 ëª©í‘œ ë‹¬ì„±!\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
